{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr           = 0.1\n",
    "\n",
    "start_epoch  = 1\n",
    "num_epochs   = 200\n",
    "batch_size   = 128\n",
    "\n",
    "is_use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if is_use_cuda else \"cpu\")\n",
    "best_acc    = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocess\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test  = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../env_pytorch/IGCV_v1/data', transform=transform_train, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='../../env_pytorch/IGCV_v1/data', transform=transform_test, train=False, download=True)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, batch_size=80, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1680,  0.1822, -0.0994, -0.0778, -0.2032, -0.0169, -0.2081,\n",
      "         -0.0093,  0.0043, -0.1735]]) <class 'torch.Tensor'> torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        reduction = 0.5\n",
    "        if 2 == stride:\n",
    "            reduction = 1\n",
    "        elif in_channels > out_channels:\n",
    "            reduction = 0.25\n",
    "            \n",
    "        self.conv1 = nn.Conv2d(in_channels, int(in_channels * reduction), 1, stride, bias=True)\n",
    "        self.bn1   = nn.BatchNorm2d(int(in_channels * reduction))\n",
    "        self.conv2 = nn.Conv2d(int(in_channels * reduction), int(in_channels * reduction * 0.5), 1, 1, bias=True)\n",
    "        self.bn2   = nn.BatchNorm2d(int(in_channels * reduction * 0.5))\n",
    "        self.conv3 = nn.Conv2d(int(in_channels * reduction * 0.5), int(in_channels * reduction), (1, 3), 1, (0, 1), bias=True)\n",
    "        self.bn3   = nn.BatchNorm2d(int(in_channels * reduction))\n",
    "        self.conv4 = nn.Conv2d(int(in_channels * reduction), int(in_channels * reduction), (3, 1), 1, (1, 0), bias=True)\n",
    "        self.bn4   = nn.BatchNorm2d(int(in_channels * reduction))\n",
    "        self.conv5 = nn.Conv2d(int(in_channels * reduction), out_channels, 1, 1, bias=True)\n",
    "        self.bn5   = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if 2 == stride or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels, out_channels, 1, stride, bias=True),\n",
    "                            nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))\n",
    "        output = F.relu(self.bn2(self.conv2(output)))\n",
    "        output = F.relu(self.bn3(self.conv3(output)))\n",
    "        output = F.relu(self.bn4(self.conv4(output)))\n",
    "        output = F.relu(self.bn5(self.conv5(output)))\n",
    "        output += F.relu(self.shortcut(input))\n",
    "        output = F.relu(output)\n",
    "        return output\n",
    "    \n",
    "class SqueezeNext(nn.Module):\n",
    "    def __init__(self, width_x, blocks, num_classes):\n",
    "        super(SqueezeNext, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1  = nn.Conv2d(3, int(width_x * self.in_channels), 3, 1, 1, bias=True)     # For Cifar10\n",
    "        #self.conv1  = nn.Conv2d(3, int(width_x * self.in_channels), 3, 2, 1, bias=True)     # For Tiny-ImageNet\n",
    "        self.bn1    = nn.BatchNorm2d(int(width_x * self.in_channels))\n",
    "        self.stage1 = self._make_layer(blocks[0], width_x, 32, 1)\n",
    "        self.stage2 = self._make_layer(blocks[1], width_x, 64, 2)\n",
    "        self.stage3 = self._make_layer(blocks[2], width_x, 128, 2)\n",
    "        self.stage4 = self._make_layer(blocks[3], width_x, 256, 2)\n",
    "        self.conv2  = nn.Conv2d(int(width_x * self.in_channels), int(width_x * 128), 1, 1, bias=True)\n",
    "        self.bn2    = nn.BatchNorm2d(int(width_x * 128))\n",
    "        self.linear = nn.Linear(int(width_x * 128), num_classes)\n",
    "        \n",
    "    def _make_layer(self, num_block, width_x, out_channels, stride):\n",
    "        strides = [stride] + [1] * (num_block - 1)\n",
    "        layers  = []\n",
    "        for _stride in strides:\n",
    "            layers.append(BasicBlock(int(width_x * self.in_channels), int(width_x * out_channels), _stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))\n",
    "        output = self.stage1(output)\n",
    "        output = self.stage2(output)\n",
    "        output = self.stage3(output)\n",
    "        output = self.stage4(output)\n",
    "        output = F.relu(self.bn2(self.conv2(output)))\n",
    "        output = F.avg_pool2d(output, 4)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "    \n",
    "def SqNxt_23_1x(num_classes):\n",
    "    return SqueezeNext(1.0, [6, 6, 8, 1], num_classes)\n",
    "\n",
    "def SqNxt_23_1x_v5(num_classes):\n",
    "    return SqueezeNext(1.0, [2, 4, 14, 1], num_classes)\n",
    "\n",
    "def SqNxt_23_2x(num_classes):\n",
    "    return SqueezeNext(2.0, [6, 6, 8, 1], num_classes)\n",
    "\n",
    "def SqNxt_23_2x_v5(num_classes):\n",
    "    return SqueezeNext(2.0, [2, 4, 14, 1], num_classes)\n",
    "\n",
    "net = SqNxt_23_1x(10)\n",
    "tmp = torch.randn(1, 3, 32, 32)\n",
    "y   = net(tmp)\n",
    "print(y, type(y), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "\n",
    "def conv_init(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('Conv') != -1:\n",
    "        init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias, 0)\n",
    "    elif class_name.find('BatchNorm') != -1:\n",
    "        init.constant_(m.weight, 1)\n",
    "        init.constant_(m.bias, 0)\n",
    "        \n",
    "net = SqNxt_23_1x(10)\n",
    "net.apply(conv_init)\n",
    "if is_use_cuda:\n",
    "    net.to(device)\n",
    "    net = nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "def lr_schedule(lr, epoch):\n",
    "    optim_factor = 0\n",
    "    if epoch > 160:\n",
    "        optim_factor = 3\n",
    "    elif epoch > 120:\n",
    "        optim_factor = 2\n",
    "    elif epoch > 60:\n",
    "        optim_factor = 1\n",
    "        \n",
    "    return lr * math.pow(0.2, optim_factor)\n",
    "\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct    = 0\n",
    "    total      = 0\n",
    "    optimizer  = optim.SGD(net.parameters(), lr=lr_schedule(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    print('Training Epoch: #%d, LR: %.4f'%(epoch, lr_schedule(lr, epoch)))\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "        if is_use_cuda:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs        = net(inputs)\n",
    "        loss           = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predict = torch.max(outputs, 1)\n",
    "        total      += labels.size(0)\n",
    "        correct    += predict.eq(labels).cpu().sum().double()\n",
    "        \n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('[%s] Training Epoch [%d/%d] Iter[%d/%d]\\t\\tLoss: %.4f Acc@1: %.3f'\n",
    "                        % (time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())),\n",
    "                           epoch, num_epochs, idx, len(train_dataset) // batch_size, \n",
    "                          train_loss / (batch_size * (idx + 1)), correct / total))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct   = 0\n",
    "    total     = 0\n",
    "    for idx, (inputs, labels) in enumerate(test_loader):\n",
    "        if is_use_cuda:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs        = net(inputs)\n",
    "        loss           = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss  += loss.item()\n",
    "        _, predict = torch.max(outputs, 1)\n",
    "        total      += labels.size(0)\n",
    "        correct    += predict.eq(labels).cpu().sum().double()\n",
    "        \n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('[%s] Testing Epoch [%d/%d] Iter[%d/%d]\\t\\tLoss: %.4f Acc@1: %.3f'\n",
    "                        % (time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())),\n",
    "                           epoch, num_epochs, idx, len(test_dataset) // 80, \n",
    "                          test_loss / (100 * (idx + 1)), correct / total))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if correct / total > best_acc:\n",
    "        print()\n",
    "        print('Saving Model...')\n",
    "        state = {\n",
    "            'net': net.module if is_use_cuda else net,\n",
    "            'acc': correct / total,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if not os.path.isdir('./checkpoint/SqNxt_23_1x'):\n",
    "            os.makedirs('./checkpoint/SqNxt_23_1x')\n",
    "        torch.save(state, './checkpoint/SqNxt_23_1x/SqNxt_23_1x_Cifar10.ckpt')\n",
    "        best_acc = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: #1, LR: 0.1000\n",
      "[2018-06-20 21:04:28] Training Epoch [1/200] Iter[390/390]\t\tLoss: 0.0121 Acc@1: 0.433\n",
      "[2018-06-20 21:04:46] Testing Epoch [1/200] Iter[124/125]\t\tLoss: 0.0150 Acc@1: 0.465\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #1 Cost 257s\n",
      "Training Epoch: #2, LR: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\envs\\pytorch0.4\\lib\\site-packages\\torch\\serialization.py:193: UserWarning: Couldn't retrieve source code for container of type SqueezeNext. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "E:\\ProgramData\\envs\\pytorch0.4\\lib\\site-packages\\torch\\serialization.py:193: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-20 21:09:48] Training Epoch [2/200] Iter[390/390]\t\tLoss: 0.0108 Acc@1: 0.497\n",
      "[2018-06-20 21:10:13] Testing Epoch [2/200] Iter[124/125]\t\tLoss: 0.0132 Acc@1: 0.517\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #2 Cost 326s\n",
      "Training Epoch: #3, LR: 0.1000\n",
      "[2018-06-20 21:16:44] Training Epoch [3/200] Iter[390/390]\t\tLoss: 0.0097 Acc@1: 0.553\n",
      "[2018-06-20 21:17:10] Testing Epoch [3/200] Iter[124/125]\t\tLoss: 0.0127 Acc@1: 0.559\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #3 Cost 416s\n",
      "Training Epoch: #4, LR: 0.1000\n",
      "[2018-06-20 21:23:33] Training Epoch [4/200] Iter[390/390]\t\tLoss: 0.0090 Acc@1: 0.590\n",
      "[2018-06-20 21:23:59] Testing Epoch [4/200] Iter[124/125]\t\tLoss: 0.0131 Acc@1: 0.547\n",
      "\n",
      "Epoch #4 Cost 408s\n",
      "Training Epoch: #5, LR: 0.1000\n",
      "[2018-06-20 21:30:10] Training Epoch [5/200] Iter[390/390]\t\tLoss: 0.0083 Acc@1: 0.622\n",
      "[2018-06-20 21:30:35] Testing Epoch [5/200] Iter[124/125]\t\tLoss: 0.0117 Acc@1: 0.580\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #5 Cost 396s\n",
      "Training Epoch: #6, LR: 0.1000\n",
      "[2018-06-20 21:36:45] Training Epoch [6/200] Iter[390/390]\t\tLoss: 0.0078 Acc@1: 0.648\n",
      "[2018-06-20 21:37:09] Testing Epoch [6/200] Iter[124/125]\t\tLoss: 0.0119 Acc@1: 0.598\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #6 Cost 394s\n",
      "Training Epoch: #7, LR: 0.1000\n",
      "[2018-06-20 21:43:18] Training Epoch [7/200] Iter[390/390]\t\tLoss: 0.0073 Acc@1: 0.668\n",
      "[2018-06-20 21:43:43] Testing Epoch [7/200] Iter[124/125]\t\tLoss: 0.0115 Acc@1: 0.615\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #7 Cost 394s\n",
      "Training Epoch: #8, LR: 0.1000\n",
      "[2018-06-20 21:49:52] Training Epoch [8/200] Iter[390/390]\t\tLoss: 0.0069 Acc@1: 0.688\n",
      "[2018-06-20 21:50:17] Testing Epoch [8/200] Iter[124/125]\t\tLoss: 0.0095 Acc@1: 0.670\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #8 Cost 393s\n",
      "Training Epoch: #9, LR: 0.1000\n",
      "[2018-06-20 21:56:27] Training Epoch [9/200] Iter[390/390]\t\tLoss: 0.0066 Acc@1: 0.699\n",
      "[2018-06-20 21:56:49] Testing Epoch [9/200] Iter[124/125]\t\tLoss: 0.0115 Acc@1: 0.633\n",
      "\n",
      "Epoch #9 Cost 392s\n",
      "Training Epoch: #10, LR: 0.1000\n",
      "[2018-06-20 22:02:59] Training Epoch [10/200] Iter[390/390]\t\tLoss: 0.0064 Acc@1: 0.714\n",
      "[2018-06-20 22:03:24] Testing Epoch [10/200] Iter[124/125]\t\tLoss: 0.0105 Acc@1: 0.660\n",
      "\n",
      "Epoch #10 Cost 395s\n",
      "Training Epoch: #11, LR: 0.1000\n",
      "[2018-06-20 22:09:37] Training Epoch [11/200] Iter[390/390]\t\tLoss: 0.0062 Acc@1: 0.723\n",
      "[2018-06-20 22:10:03] Testing Epoch [11/200] Iter[124/125]\t\tLoss: 0.0119 Acc@1: 0.610\n",
      "\n",
      "Epoch #11 Cost 398s\n",
      "Training Epoch: #12, LR: 0.1000\n",
      "[2018-06-20 22:16:18] Training Epoch [12/200] Iter[390/390]\t\tLoss: 0.0060 Acc@1: 0.733\n",
      "[2018-06-20 22:16:44] Testing Epoch [12/200] Iter[124/125]\t\tLoss: 0.0090 Acc@1: 0.693\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #12 Cost 400s\n",
      "Training Epoch: #13, LR: 0.1000\n",
      "[2018-06-20 22:22:58] Training Epoch [13/200] Iter[390/390]\t\tLoss: 0.0058 Acc@1: 0.738\n",
      "[2018-06-20 22:23:23] Testing Epoch [13/200] Iter[124/125]\t\tLoss: 0.0089 Acc@1: 0.717\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #13 Cost 399s\n",
      "Training Epoch: #14, LR: 0.1000\n",
      "[2018-06-20 22:29:32] Training Epoch [14/200] Iter[390/390]\t\tLoss: 0.0057 Acc@1: 0.749\n",
      "[2018-06-20 22:29:58] Testing Epoch [14/200] Iter[124/125]\t\tLoss: 0.0083 Acc@1: 0.713\n",
      "\n",
      "Epoch #14 Cost 394s\n",
      "Training Epoch: #15, LR: 0.1000\n",
      "[2018-06-20 22:36:10] Training Epoch [15/200] Iter[390/390]\t\tLoss: 0.0056 Acc@1: 0.753\n",
      "[2018-06-20 22:36:35] Testing Epoch [15/200] Iter[124/125]\t\tLoss: 0.0085 Acc@1: 0.707\n",
      "\n",
      "Epoch #15 Cost 397s\n",
      "Training Epoch: #16, LR: 0.1000\n",
      "[2018-06-20 22:42:45] Training Epoch [16/200] Iter[390/390]\t\tLoss: 0.0055 Acc@1: 0.757\n",
      "[2018-06-20 22:43:11] Testing Epoch [16/200] Iter[124/125]\t\tLoss: 0.0087 Acc@1: 0.714\n",
      "\n",
      "Epoch #16 Cost 395s\n",
      "Training Epoch: #17, LR: 0.1000\n",
      "[2018-06-20 22:49:19] Training Epoch [17/200] Iter[390/390]\t\tLoss: 0.0054 Acc@1: 0.758\n",
      "[2018-06-20 22:49:44] Testing Epoch [17/200] Iter[124/125]\t\tLoss: 0.0082 Acc@1: 0.725\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #17 Cost 393s\n",
      "Training Epoch: #18, LR: 0.1000\n",
      "[2018-06-20 22:55:56] Training Epoch [18/200] Iter[390/390]\t\tLoss: 0.0054 Acc@1: 0.761\n",
      "[2018-06-20 22:56:20] Testing Epoch [18/200] Iter[124/125]\t\tLoss: 0.0102 Acc@1: 0.687\n",
      "\n",
      "Epoch #18 Cost 395s\n",
      "Training Epoch: #19, LR: 0.1000\n",
      "[2018-06-20 23:02:30] Training Epoch [19/200] Iter[390/390]\t\tLoss: 0.0053 Acc@1: 0.767\n",
      "[2018-06-20 23:02:53] Testing Epoch [19/200] Iter[124/125]\t\tLoss: 0.0080 Acc@1: 0.730\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #19 Cost 392s\n",
      "Training Epoch: #20, LR: 0.1000\n",
      "[2018-06-20 23:09:02] Training Epoch [20/200] Iter[390/390]\t\tLoss: 0.0052 Acc@1: 0.769\n",
      "[2018-06-20 23:09:28] Testing Epoch [20/200] Iter[124/125]\t\tLoss: 0.0071 Acc@1: 0.755\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #20 Cost 394s\n",
      "Training Epoch: #21, LR: 0.1000\n",
      "[2018-06-20 23:15:39] Training Epoch [21/200] Iter[390/390]\t\tLoss: 0.0052 Acc@1: 0.772\n",
      "[2018-06-20 23:16:04] Testing Epoch [21/200] Iter[124/125]\t\tLoss: 0.0083 Acc@1: 0.724\n",
      "\n",
      "Epoch #21 Cost 396s\n",
      "Training Epoch: #22, LR: 0.1000\n",
      "[2018-06-20 23:22:17] Training Epoch [22/200] Iter[390/390]\t\tLoss: 0.0051 Acc@1: 0.774\n",
      "[2018-06-20 23:22:42] Testing Epoch [22/200] Iter[124/125]\t\tLoss: 0.0098 Acc@1: 0.678\n",
      "\n",
      "Epoch #22 Cost 397s\n",
      "Training Epoch: #23, LR: 0.1000\n",
      "[2018-06-20 23:28:53] Training Epoch [23/200] Iter[390/390]\t\tLoss: 0.0051 Acc@1: 0.772\n",
      "[2018-06-20 23:29:19] Testing Epoch [23/200] Iter[124/125]\t\tLoss: 0.0075 Acc@1: 0.745\n",
      "\n",
      "Epoch #23 Cost 396s\n",
      "Training Epoch: #24, LR: 0.1000\n",
      "[2018-06-20 23:35:30] Training Epoch [24/200] Iter[390/390]\t\tLoss: 0.0051 Acc@1: 0.774\n",
      "[2018-06-20 23:35:56] Testing Epoch [24/200] Iter[124/125]\t\tLoss: 0.0076 Acc@1: 0.752\n",
      "\n",
      "Epoch #24 Cost 397s\n",
      "Training Epoch: #25, LR: 0.1000\n",
      "[2018-06-20 23:42:06] Training Epoch [25/200] Iter[390/390]\t\tLoss: 0.0050 Acc@1: 0.779\n",
      "[2018-06-20 23:42:31] Testing Epoch [25/200] Iter[124/125]\t\tLoss: 0.0084 Acc@1: 0.723\n",
      "\n",
      "Epoch #25 Cost 395s\n",
      "Training Epoch: #26, LR: 0.1000\n",
      "[2018-06-20 23:48:40] Training Epoch [26/200] Iter[390/390]\t\tLoss: 0.0050 Acc@1: 0.776\n",
      "[2018-06-20 23:49:05] Testing Epoch [26/200] Iter[124/125]\t\tLoss: 0.0080 Acc@1: 0.732\n",
      "\n",
      "Epoch #26 Cost 393s\n",
      "Training Epoch: #27, LR: 0.1000\n",
      "[2018-06-20 23:55:14] Training Epoch [27/200] Iter[390/390]\t\tLoss: 0.0050 Acc@1: 0.779\n",
      "[2018-06-20 23:55:40] Testing Epoch [27/200] Iter[124/125]\t\tLoss: 0.0088 Acc@1: 0.711\n",
      "\n",
      "Epoch #27 Cost 395s\n",
      "Training Epoch: #28, LR: 0.1000\n",
      "[2018-06-21 00:01:51] Training Epoch [28/200] Iter[390/390]\t\tLoss: 0.0050 Acc@1: 0.779\n",
      "[2018-06-21 00:02:16] Testing Epoch [28/200] Iter[124/125]\t\tLoss: 0.0087 Acc@1: 0.716\n",
      "\n",
      "Epoch #28 Cost 396s\n",
      "Training Epoch: #29, LR: 0.1000\n",
      "[2018-06-21 00:08:22] Training Epoch [29/200] Iter[390/390]\t\tLoss: 0.0050 Acc@1: 0.779\n",
      "[2018-06-21 00:08:46] Testing Epoch [29/200] Iter[124/125]\t\tLoss: 0.0098 Acc@1: 0.681\n",
      "\n",
      "Epoch #29 Cost 390s\n",
      "Training Epoch: #30, LR: 0.1000\n",
      "[2018-06-21 00:14:56] Training Epoch [30/200] Iter[390/390]\t\tLoss: 0.0049 Acc@1: 0.784\n",
      "[2018-06-21 00:15:18] Testing Epoch [30/200] Iter[124/125]\t\tLoss: 0.0098 Acc@1: 0.662\n",
      "\n",
      "Epoch #30 Cost 391s\n",
      "Training Epoch: #31, LR: 0.1000\n",
      "[2018-06-21 00:21:27] Training Epoch [31/200] Iter[390/390]\t\tLoss: 0.0049 Acc@1: 0.783\n",
      "[2018-06-21 00:21:53] Testing Epoch [31/200] Iter[124/125]\t\tLoss: 0.0075 Acc@1: 0.745\n",
      "\n",
      "Epoch #31 Cost 394s\n",
      "Training Epoch: #32, LR: 0.1000\n",
      "[2018-06-21 00:28:06] Training Epoch [32/200] Iter[390/390]\t\tLoss: 0.0049 Acc@1: 0.783\n",
      "[2018-06-21 00:28:31] Testing Epoch [32/200] Iter[124/125]\t\tLoss: 0.0126 Acc@1: 0.621\n",
      "\n",
      "Epoch #32 Cost 398s\n",
      "Training Epoch: #33, LR: 0.1000\n",
      "[2018-06-21 00:34:42] Training Epoch [33/200] Iter[390/390]\t\tLoss: 0.0049 Acc@1: 0.786\n",
      "[2018-06-21 00:35:07] Testing Epoch [33/200] Iter[124/125]\t\tLoss: 0.0087 Acc@1: 0.707\n",
      "\n",
      "Epoch #33 Cost 395s\n",
      "Training Epoch: #34, LR: 0.1000\n",
      "[2018-06-21 00:41:17] Training Epoch [34/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.785\n",
      "[2018-06-21 00:41:42] Testing Epoch [34/200] Iter[124/125]\t\tLoss: 0.0089 Acc@1: 0.697\n",
      "\n",
      "Epoch #34 Cost 395s\n",
      "Training Epoch: #35, LR: 0.1000\n",
      "[2018-06-21 00:47:52] Training Epoch [35/200] Iter[390/390]\t\tLoss: 0.0049 Acc@1: 0.783\n",
      "[2018-06-21 00:48:18] Testing Epoch [35/200] Iter[124/125]\t\tLoss: 0.0094 Acc@1: 0.690\n",
      "\n",
      "Epoch #35 Cost 396s\n",
      "Training Epoch: #36, LR: 0.1000\n",
      "[2018-06-21 00:54:30] Training Epoch [36/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.787\n",
      "[2018-06-21 00:54:55] Testing Epoch [36/200] Iter[124/125]\t\tLoss: 0.0074 Acc@1: 0.754\n",
      "\n",
      "Epoch #36 Cost 397s\n",
      "Training Epoch: #37, LR: 0.1000\n",
      "[2018-06-21 01:01:05] Training Epoch [37/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.786\n",
      "[2018-06-21 01:01:30] Testing Epoch [37/200] Iter[124/125]\t\tLoss: 0.0072 Acc@1: 0.756\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #37 Cost 394s\n",
      "Training Epoch: #38, LR: 0.1000\n",
      "[2018-06-21 01:07:39] Training Epoch [38/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.785\n",
      "[2018-06-21 01:08:03] Testing Epoch [38/200] Iter[124/125]\t\tLoss: 0.0115 Acc@1: 0.653\n",
      "\n",
      "Epoch #38 Cost 393s\n",
      "Training Epoch: #39, LR: 0.1000\n",
      "[2018-06-21 01:14:16] Training Epoch [39/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.787\n",
      "[2018-06-21 01:14:45] Testing Epoch [39/200] Iter[124/125]\t\tLoss: 0.0078 Acc@1: 0.741\n",
      "\n",
      "Epoch #39 Cost 401s\n",
      "Training Epoch: #40, LR: 0.1000\n",
      "[2018-06-21 01:20:55] Training Epoch [40/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.789\n",
      "[2018-06-21 01:21:20] Testing Epoch [40/200] Iter[124/125]\t\tLoss: 0.0069 Acc@1: 0.757\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #40 Cost 394s\n",
      "Training Epoch: #41, LR: 0.1000\n",
      "[2018-06-21 01:27:30] Training Epoch [41/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.788\n",
      "[2018-06-21 01:27:53] Testing Epoch [41/200] Iter[124/125]\t\tLoss: 0.0101 Acc@1: 0.676\n",
      "\n",
      "Epoch #41 Cost 392s\n",
      "Training Epoch: #42, LR: 0.1000\n",
      "[2018-06-21 01:34:09] Training Epoch [42/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.788\n",
      "[2018-06-21 01:34:34] Testing Epoch [42/200] Iter[124/125]\t\tLoss: 0.0077 Acc@1: 0.731\n",
      "\n",
      "Epoch #42 Cost 400s\n",
      "Training Epoch: #43, LR: 0.1000\n",
      "[2018-06-21 01:40:46] Training Epoch [43/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.790\n",
      "[2018-06-21 01:41:12] Testing Epoch [43/200] Iter[124/125]\t\tLoss: 0.0089 Acc@1: 0.700\n",
      "\n",
      "Epoch #43 Cost 397s\n",
      "Training Epoch: #44, LR: 0.1000\n",
      "[2018-06-21 01:47:24] Training Epoch [44/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.791\n",
      "[2018-06-21 01:47:48] Testing Epoch [44/200] Iter[124/125]\t\tLoss: 0.0090 Acc@1: 0.714\n",
      "\n",
      "Epoch #44 Cost 396s\n",
      "Training Epoch: #45, LR: 0.1000\n",
      "[2018-06-21 01:53:58] Training Epoch [45/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.792\n",
      "[2018-06-21 01:54:23] Testing Epoch [45/200] Iter[124/125]\t\tLoss: 0.0088 Acc@1: 0.706\n",
      "\n",
      "Epoch #45 Cost 394s\n",
      "Training Epoch: #46, LR: 0.1000\n",
      "[2018-06-21 02:00:33] Training Epoch [46/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.790\n",
      "[2018-06-21 02:00:58] Testing Epoch [46/200] Iter[124/125]\t\tLoss: 0.0085 Acc@1: 0.716\n",
      "\n",
      "Epoch #46 Cost 395s\n",
      "Training Epoch: #47, LR: 0.1000\n",
      "[2018-06-21 02:07:13] Training Epoch [47/200] Iter[390/390]\t\tLoss: 0.0048 Acc@1: 0.789\n",
      "[2018-06-21 02:07:38] Testing Epoch [47/200] Iter[124/125]\t\tLoss: 0.0081 Acc@1: 0.730\n",
      "\n",
      "Epoch #47 Cost 399s\n",
      "Training Epoch: #48, LR: 0.1000\n",
      "[2018-06-21 02:13:50] Training Epoch [48/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.793\n",
      "[2018-06-21 02:14:15] Testing Epoch [48/200] Iter[124/125]\t\tLoss: 0.0091 Acc@1: 0.696\n",
      "\n",
      "Epoch #48 Cost 396s\n",
      "Training Epoch: #49, LR: 0.1000\n",
      "[2018-06-21 02:20:28] Training Epoch [49/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.790\n",
      "[2018-06-21 02:20:53] Testing Epoch [49/200] Iter[124/125]\t\tLoss: 0.0071 Acc@1: 0.750\n",
      "\n",
      "Epoch #49 Cost 398s\n",
      "Training Epoch: #50, LR: 0.1000\n",
      "[2018-06-21 02:27:05] Training Epoch [50/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.793\n",
      "[2018-06-21 02:27:29] Testing Epoch [50/200] Iter[124/125]\t\tLoss: 0.0090 Acc@1: 0.694\n",
      "\n",
      "Epoch #50 Cost 395s\n",
      "Training Epoch: #51, LR: 0.1000\n",
      "[2018-06-21 02:33:43] Training Epoch [51/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.795\n",
      "[2018-06-21 02:34:07] Testing Epoch [51/200] Iter[124/125]\t\tLoss: 0.0085 Acc@1: 0.710\n",
      "\n",
      "Epoch #51 Cost 398s\n",
      "Training Epoch: #52, LR: 0.1000\n",
      "[2018-06-21 02:40:18] Training Epoch [52/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.791\n",
      "[2018-06-21 02:40:43] Testing Epoch [52/200] Iter[124/125]\t\tLoss: 0.0073 Acc@1: 0.749\n",
      "\n",
      "Epoch #52 Cost 395s\n",
      "Training Epoch: #53, LR: 0.1000\n",
      "[2018-06-21 02:46:58] Training Epoch [53/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.792\n",
      "[2018-06-21 02:47:23] Testing Epoch [53/200] Iter[124/125]\t\tLoss: 0.0087 Acc@1: 0.713\n",
      "\n",
      "Epoch #53 Cost 400s\n",
      "Training Epoch: #54, LR: 0.1000\n",
      "[2018-06-21 02:53:38] Training Epoch [54/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.794\n",
      "[2018-06-21 02:54:03] Testing Epoch [54/200] Iter[124/125]\t\tLoss: 0.0091 Acc@1: 0.690\n",
      "\n",
      "Epoch #54 Cost 399s\n",
      "Training Epoch: #55, LR: 0.1000\n",
      "[2018-06-21 03:00:13] Training Epoch [55/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.794\n",
      "[2018-06-21 03:00:38] Testing Epoch [55/200] Iter[124/125]\t\tLoss: 0.0070 Acc@1: 0.758\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #55 Cost 395s\n",
      "Training Epoch: #56, LR: 0.1000\n",
      "[2018-06-21 03:06:50] Training Epoch [56/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.792\n",
      "[2018-06-21 03:07:15] Testing Epoch [56/200] Iter[124/125]\t\tLoss: 0.0077 Acc@1: 0.742\n",
      "\n",
      "Epoch #56 Cost 396s\n",
      "Training Epoch: #57, LR: 0.1000\n",
      "[2018-06-21 03:13:28] Training Epoch [57/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.795\n",
      "[2018-06-21 03:13:53] Testing Epoch [57/200] Iter[124/125]\t\tLoss: 0.0092 Acc@1: 0.708\n",
      "\n",
      "Epoch #57 Cost 398s\n",
      "Training Epoch: #58, LR: 0.1000\n",
      "[2018-06-21 03:20:01] Training Epoch [58/200] Iter[390/390]\t\tLoss: 0.0047 Acc@1: 0.793\n",
      "[2018-06-21 03:20:25] Testing Epoch [58/200] Iter[124/125]\t\tLoss: 0.0098 Acc@1: 0.698\n",
      "\n",
      "Epoch #58 Cost 392s\n",
      "Training Epoch: #59, LR: 0.1000\n",
      "[2018-06-21 03:26:42] Training Epoch [59/200] Iter[390/390]\t\tLoss: 0.0046 Acc@1: 0.794\n",
      "[2018-06-21 03:27:08] Testing Epoch [59/200] Iter[124/125]\t\tLoss: 0.0098 Acc@1: 0.692\n",
      "\n",
      "Epoch #59 Cost 402s\n",
      "Training Epoch: #60, LR: 0.1000\n",
      "[2018-06-21 03:33:15] Training Epoch [60/200] Iter[390/390]\t\tLoss: 0.0046 Acc@1: 0.796\n",
      "[2018-06-21 03:33:40] Testing Epoch [60/200] Iter[124/125]\t\tLoss: 0.0090 Acc@1: 0.714\n",
      "\n",
      "Epoch #60 Cost 392s\n",
      "Training Epoch: #61, LR: 0.0200\n",
      "[2018-06-21 03:39:52] Training Epoch [61/200] Iter[390/390]\t\tLoss: 0.0033 Acc@1: 0.855\n",
      "[2018-06-21 03:40:18] Testing Epoch [61/200] Iter[124/125]\t\tLoss: 0.0041 Acc@1: 0.863\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #61 Cost 398s\n",
      "Training Epoch: #62, LR: 0.0200\n",
      "[2018-06-21 03:46:27] Training Epoch [62/200] Iter[390/390]\t\tLoss: 0.0029 Acc@1: 0.873\n",
      "[2018-06-21 03:46:53] Testing Epoch [62/200] Iter[124/125]\t\tLoss: 0.0041 Acc@1: 0.862\n",
      "\n",
      "Epoch #62 Cost 395s\n",
      "Training Epoch: #63, LR: 0.0200\n",
      "[2018-06-21 03:53:02] Training Epoch [63/200] Iter[390/390]\t\tLoss: 0.0027 Acc@1: 0.878\n",
      "[2018-06-21 03:53:28] Testing Epoch [63/200] Iter[124/125]\t\tLoss: 0.0041 Acc@1: 0.864\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #63 Cost 395s\n",
      "Training Epoch: #64, LR: 0.0200\n",
      "[2018-06-21 03:59:38] Training Epoch [64/200] Iter[390/390]\t\tLoss: 0.0027 Acc@1: 0.882\n",
      "[2018-06-21 04:00:03] Testing Epoch [64/200] Iter[124/125]\t\tLoss: 0.0042 Acc@1: 0.858\n",
      "\n",
      "Epoch #64 Cost 394s\n",
      "Training Epoch: #65, LR: 0.0200\n",
      "[2018-06-21 04:06:12] Training Epoch [65/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.884\n",
      "[2018-06-21 04:06:38] Testing Epoch [65/200] Iter[124/125]\t\tLoss: 0.0040 Acc@1: 0.868\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #65 Cost 395s\n",
      "Training Epoch: #66, LR: 0.0200\n",
      "[2018-06-21 04:12:51] Training Epoch [66/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.884\n",
      "[2018-06-21 04:13:16] Testing Epoch [66/200] Iter[124/125]\t\tLoss: 0.0048 Acc@1: 0.841\n",
      "\n",
      "Epoch #66 Cost 397s\n",
      "Training Epoch: #67, LR: 0.0200\n",
      "[2018-06-21 04:19:51] Training Epoch [67/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.884\n",
      "[2018-06-21 04:20:16] Testing Epoch [67/200] Iter[124/125]\t\tLoss: 0.0042 Acc@1: 0.858\n",
      "\n",
      "Epoch #67 Cost 420s\n",
      "Training Epoch: #68, LR: 0.0200\n",
      "[2018-06-21 04:26:19] Training Epoch [68/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.887\n",
      "[2018-06-21 04:26:44] Testing Epoch [68/200] Iter[124/125]\t\tLoss: 0.0042 Acc@1: 0.858\n",
      "\n",
      "Epoch #68 Cost 388s\n",
      "Training Epoch: #69, LR: 0.0200\n",
      "[2018-06-21 04:32:52] Training Epoch [69/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.888\n",
      "[2018-06-21 04:33:17] Testing Epoch [69/200] Iter[124/125]\t\tLoss: 0.0041 Acc@1: 0.861\n",
      "\n",
      "Epoch #69 Cost 393s\n",
      "Training Epoch: #70, LR: 0.0200\n",
      "[2018-06-21 04:39:23] Training Epoch [70/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.888\n",
      "[2018-06-21 04:39:48] Testing Epoch [70/200] Iter[124/125]\t\tLoss: 0.0044 Acc@1: 0.851\n",
      "\n",
      "Epoch #70 Cost 390s\n",
      "Training Epoch: #71, LR: 0.0200\n",
      "[2018-06-21 04:45:59] Training Epoch [71/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.888\n",
      "[2018-06-21 04:46:24] Testing Epoch [71/200] Iter[124/125]\t\tLoss: 0.0045 Acc@1: 0.849\n",
      "\n",
      "Epoch #71 Cost 395s\n",
      "Training Epoch: #72, LR: 0.0200\n",
      "[2018-06-21 04:52:35] Training Epoch [72/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.889\n",
      "[2018-06-21 04:53:00] Testing Epoch [72/200] Iter[124/125]\t\tLoss: 0.0044 Acc@1: 0.850\n",
      "\n",
      "Epoch #72 Cost 396s\n",
      "Training Epoch: #73, LR: 0.0200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-21 04:59:11] Training Epoch [73/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.887\n",
      "[2018-06-21 04:59:36] Testing Epoch [73/200] Iter[124/125]\t\tLoss: 0.0040 Acc@1: 0.864\n",
      "\n",
      "Epoch #73 Cost 396s\n",
      "Training Epoch: #74, LR: 0.0200\n",
      "[2018-06-21 05:05:43] Training Epoch [74/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.885\n",
      "[2018-06-21 05:06:08] Testing Epoch [74/200] Iter[124/125]\t\tLoss: 0.0048 Acc@1: 0.839\n",
      "\n",
      "Epoch #74 Cost 392s\n",
      "Training Epoch: #75, LR: 0.0200\n",
      "[2018-06-21 05:12:17] Training Epoch [75/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.887\n",
      "[2018-06-21 05:12:43] Testing Epoch [75/200] Iter[124/125]\t\tLoss: 0.0045 Acc@1: 0.848\n",
      "\n",
      "Epoch #75 Cost 394s\n",
      "Training Epoch: #76, LR: 0.0200\n",
      "[2018-06-21 05:18:52] Training Epoch [76/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.886\n",
      "[2018-06-21 05:19:17] Testing Epoch [76/200] Iter[124/125]\t\tLoss: 0.0045 Acc@1: 0.848\n",
      "\n",
      "Epoch #76 Cost 394s\n",
      "Training Epoch: #77, LR: 0.0200\n",
      "[2018-06-21 05:25:26] Training Epoch [77/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.885\n",
      "[2018-06-21 05:25:50] Testing Epoch [77/200] Iter[124/125]\t\tLoss: 0.0044 Acc@1: 0.855\n",
      "\n",
      "Epoch #77 Cost 393s\n",
      "Training Epoch: #78, LR: 0.0200\n",
      "[2018-06-21 05:32:01] Training Epoch [78/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.886\n",
      "[2018-06-21 05:32:27] Testing Epoch [78/200] Iter[124/125]\t\tLoss: 0.0051 Acc@1: 0.832\n",
      "\n",
      "Epoch #78 Cost 396s\n",
      "Training Epoch: #79, LR: 0.0200\n",
      "[2018-06-21 05:38:35] Training Epoch [79/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.887\n",
      "[2018-06-21 05:39:00] Testing Epoch [79/200] Iter[124/125]\t\tLoss: 0.0051 Acc@1: 0.833\n",
      "\n",
      "Epoch #79 Cost 393s\n",
      "Training Epoch: #80, LR: 0.0200\n",
      "[2018-06-21 05:45:09] Training Epoch [80/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.887\n",
      "[2018-06-21 05:45:34] Testing Epoch [80/200] Iter[124/125]\t\tLoss: 0.0057 Acc@1: 0.814\n",
      "\n",
      "Epoch #80 Cost 393s\n",
      "Training Epoch: #81, LR: 0.0200\n",
      "[2018-06-21 05:51:43] Training Epoch [81/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.884\n",
      "[2018-06-21 05:52:05] Testing Epoch [81/200] Iter[124/125]\t\tLoss: 0.0047 Acc@1: 0.842\n",
      "\n",
      "Epoch #81 Cost 391s\n",
      "Training Epoch: #82, LR: 0.0200\n",
      "[2018-06-21 05:58:14] Training Epoch [82/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.883\n",
      "[2018-06-21 05:58:39] Testing Epoch [82/200] Iter[124/125]\t\tLoss: 0.0047 Acc@1: 0.845\n",
      "\n",
      "Epoch #82 Cost 393s\n",
      "Training Epoch: #83, LR: 0.0200\n",
      "[2018-06-21 06:04:53] Training Epoch [83/200] Iter[390/390]\t\tLoss: 0.0026 Acc@1: 0.884\n",
      "[2018-06-21 06:05:19] Testing Epoch [83/200] Iter[124/125]\t\tLoss: 0.0051 Acc@1: 0.832\n",
      "\n",
      "Epoch #83 Cost 399s\n",
      "Training Epoch: #84, LR: 0.0200\n",
      "[2018-06-21 06:11:30] Training Epoch [84/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.889\n",
      "[2018-06-21 06:11:55] Testing Epoch [84/200] Iter[124/125]\t\tLoss: 0.0048 Acc@1: 0.837\n",
      "\n",
      "Epoch #84 Cost 396s\n",
      "Training Epoch: #85, LR: 0.0200\n",
      "[2018-06-21 06:18:04] Training Epoch [85/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.887\n",
      "[2018-06-21 06:18:29] Testing Epoch [85/200] Iter[124/125]\t\tLoss: 0.0042 Acc@1: 0.860\n",
      "\n",
      "Epoch #85 Cost 393s\n",
      "Training Epoch: #86, LR: 0.0200\n",
      "[2018-06-21 06:24:40] Training Epoch [86/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.888\n",
      "[2018-06-21 06:25:05] Testing Epoch [86/200] Iter[124/125]\t\tLoss: 0.0048 Acc@1: 0.840\n",
      "\n",
      "Epoch #86 Cost 396s\n",
      "Training Epoch: #87, LR: 0.0200\n",
      "[2018-06-21 06:31:17] Training Epoch [87/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.887\n",
      "[2018-06-21 06:31:42] Testing Epoch [87/200] Iter[124/125]\t\tLoss: 0.0044 Acc@1: 0.852\n",
      "\n",
      "Epoch #87 Cost 396s\n",
      "Training Epoch: #88, LR: 0.0200\n",
      "[2018-06-21 06:37:57] Training Epoch [88/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.890\n",
      "[2018-06-21 06:38:22] Testing Epoch [88/200] Iter[124/125]\t\tLoss: 0.0044 Acc@1: 0.856\n",
      "\n",
      "Epoch #88 Cost 400s\n",
      "Training Epoch: #89, LR: 0.0200\n",
      "[2018-06-21 06:44:31] Training Epoch [89/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.888\n",
      "[2018-06-21 06:44:56] Testing Epoch [89/200] Iter[124/125]\t\tLoss: 0.0048 Acc@1: 0.838\n",
      "\n",
      "Epoch #89 Cost 393s\n",
      "Training Epoch: #90, LR: 0.0200\n",
      "[2018-06-21 06:51:03] Training Epoch [90/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.889\n",
      "[2018-06-21 06:51:28] Testing Epoch [90/200] Iter[124/125]\t\tLoss: 0.0048 Acc@1: 0.843\n",
      "\n",
      "Epoch #90 Cost 392s\n",
      "Training Epoch: #91, LR: 0.0200\n",
      "[2018-06-21 06:57:41] Training Epoch [91/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.889\n",
      "[2018-06-21 06:58:04] Testing Epoch [91/200] Iter[124/125]\t\tLoss: 0.0046 Acc@1: 0.846\n",
      "\n",
      "Epoch #91 Cost 396s\n",
      "Training Epoch: #92, LR: 0.0200\n",
      "[2018-06-21 07:04:12] Training Epoch [92/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.889\n",
      "[2018-06-21 07:04:36] Testing Epoch [92/200] Iter[124/125]\t\tLoss: 0.0050 Acc@1: 0.832\n",
      "\n",
      "Epoch #92 Cost 391s\n",
      "Training Epoch: #93, LR: 0.0200\n",
      "[2018-06-21 07:10:48] Training Epoch [93/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.890\n",
      "[2018-06-21 07:11:12] Testing Epoch [93/200] Iter[124/125]\t\tLoss: 0.0050 Acc@1: 0.831\n",
      "\n",
      "Epoch #93 Cost 396s\n",
      "Training Epoch: #94, LR: 0.0200\n",
      "[2018-06-21 07:17:22] Training Epoch [94/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.891\n",
      "[2018-06-21 07:17:48] Testing Epoch [94/200] Iter[124/125]\t\tLoss: 0.0046 Acc@1: 0.842\n",
      "\n",
      "Epoch #94 Cost 395s\n",
      "Training Epoch: #95, LR: 0.0200\n",
      "[2018-06-21 07:23:57] Training Epoch [95/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.891\n",
      "[2018-06-21 07:24:21] Testing Epoch [95/200] Iter[124/125]\t\tLoss: 0.0059 Acc@1: 0.810\n",
      "\n",
      "Epoch #95 Cost 393s\n",
      "Training Epoch: #96, LR: 0.0200\n",
      "[2018-06-21 07:30:33] Training Epoch [96/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.890\n",
      "[2018-06-21 07:30:58] Testing Epoch [96/200] Iter[124/125]\t\tLoss: 0.0046 Acc@1: 0.844\n",
      "\n",
      "Epoch #96 Cost 396s\n",
      "Training Epoch: #97, LR: 0.0200\n",
      "[2018-06-21 07:37:08] Training Epoch [97/200] Iter[390/390]\t\tLoss: 0.0025 Acc@1: 0.890\n",
      "[2018-06-21 07:37:32] Testing Epoch [97/200] Iter[124/125]\t\tLoss: 0.0047 Acc@1: 0.848\n",
      "\n",
      "Epoch #97 Cost 394s\n",
      "Training Epoch: #98, LR: 0.0200\n",
      "[2018-06-21 07:43:43] Training Epoch [98/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.891\n",
      "[2018-06-21 07:44:08] Testing Epoch [98/200] Iter[124/125]\t\tLoss: 0.0046 Acc@1: 0.850\n",
      "\n",
      "Epoch #98 Cost 395s\n",
      "Training Epoch: #99, LR: 0.0200\n",
      "[2018-06-21 07:50:17] Training Epoch [99/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.895\n",
      "[2018-06-21 07:50:42] Testing Epoch [99/200] Iter[124/125]\t\tLoss: 0.0051 Acc@1: 0.831\n",
      "\n",
      "Epoch #99 Cost 394s\n",
      "Training Epoch: #100, LR: 0.0200\n",
      "[2018-06-21 07:56:59] Training Epoch [100/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.893\n",
      "[2018-06-21 07:57:24] Testing Epoch [100/200] Iter[124/125]\t\tLoss: 0.0045 Acc@1: 0.852\n",
      "\n",
      "Epoch #100 Cost 402s\n",
      "Training Epoch: #101, LR: 0.0200\n",
      "[2018-06-21 08:03:33] Training Epoch [101/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.891\n",
      "[2018-06-21 08:03:58] Testing Epoch [101/200] Iter[124/125]\t\tLoss: 0.0046 Acc@1: 0.846\n",
      "\n",
      "Epoch #101 Cost 393s\n",
      "Training Epoch: #102, LR: 0.0200\n",
      "[2018-06-21 08:10:06] Training Epoch [102/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.892\n",
      "[2018-06-21 08:10:31] Testing Epoch [102/200] Iter[124/125]\t\tLoss: 0.0050 Acc@1: 0.835\n",
      "\n",
      "Epoch #102 Cost 393s\n",
      "Training Epoch: #103, LR: 0.0200\n",
      "[2018-06-21 08:16:47] Training Epoch [103/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.894\n",
      "[2018-06-21 08:17:12] Testing Epoch [103/200] Iter[124/125]\t\tLoss: 0.0044 Acc@1: 0.855\n",
      "\n",
      "Epoch #103 Cost 400s\n",
      "Training Epoch: #104, LR: 0.0200\n",
      "[2018-06-21 08:23:22] Training Epoch [104/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.894\n",
      "[2018-06-21 08:23:47] Testing Epoch [104/200] Iter[124/125]\t\tLoss: 0.0047 Acc@1: 0.848\n",
      "\n",
      "Epoch #104 Cost 394s\n",
      "Training Epoch: #105, LR: 0.0200\n",
      "[2018-06-21 08:29:59] Training Epoch [105/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.893\n",
      "[2018-06-21 08:30:24] Testing Epoch [105/200] Iter[124/125]\t\tLoss: 0.0040 Acc@1: 0.864\n",
      "\n",
      "Epoch #105 Cost 397s\n",
      "Training Epoch: #106, LR: 0.0200\n",
      "[2018-06-21 08:36:36] Training Epoch [106/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.896\n",
      "[2018-06-21 08:37:01] Testing Epoch [106/200] Iter[124/125]\t\tLoss: 0.0049 Acc@1: 0.845\n",
      "\n",
      "Epoch #106 Cost 397s\n",
      "Training Epoch: #107, LR: 0.0200\n",
      "[2018-06-21 08:43:11] Training Epoch [107/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.894\n",
      "[2018-06-21 08:43:36] Testing Epoch [107/200] Iter[124/125]\t\tLoss: 0.0048 Acc@1: 0.842\n",
      "\n",
      "Epoch #107 Cost 395s\n",
      "Training Epoch: #108, LR: 0.0200\n",
      "[2018-06-21 08:49:42] Training Epoch [108/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.895\n",
      "[2018-06-21 08:50:08] Testing Epoch [108/200] Iter[124/125]\t\tLoss: 0.0055 Acc@1: 0.831\n",
      "\n",
      "Epoch #108 Cost 391s\n",
      "Training Epoch: #109, LR: 0.0200\n",
      "[2018-06-21 08:56:19] Training Epoch [109/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.894\n",
      "[2018-06-21 08:56:44] Testing Epoch [109/200] Iter[124/125]\t\tLoss: 0.0046 Acc@1: 0.851\n",
      "\n",
      "Epoch #109 Cost 395s\n",
      "Training Epoch: #110, LR: 0.0200\n",
      "[2018-06-21 09:02:53] Training Epoch [110/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.896\n",
      "[2018-06-21 09:03:19] Testing Epoch [110/200] Iter[124/125]\t\tLoss: 0.0049 Acc@1: 0.839\n",
      "\n",
      "Epoch #110 Cost 394s\n",
      "Training Epoch: #111, LR: 0.0200\n",
      "[2018-06-21 09:09:29] Training Epoch [111/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.896\n",
      "[2018-06-21 09:09:53] Testing Epoch [111/200] Iter[124/125]\t\tLoss: 0.0045 Acc@1: 0.853\n",
      "\n",
      "Epoch #111 Cost 394s\n",
      "Training Epoch: #112, LR: 0.0200\n",
      "[2018-06-21 09:16:08] Training Epoch [112/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.896\n",
      "[2018-06-21 09:16:31] Testing Epoch [112/200] Iter[124/125]\t\tLoss: 0.0051 Acc@1: 0.835\n",
      "\n",
      "Epoch #112 Cost 397s\n",
      "Training Epoch: #113, LR: 0.0200\n",
      "[2018-06-21 09:22:44] Training Epoch [113/200] Iter[390/390]\t\tLoss: 0.0024 Acc@1: 0.895\n",
      "[2018-06-21 09:23:09] Testing Epoch [113/200] Iter[124/125]\t\tLoss: 0.0047 Acc@1: 0.848\n",
      "\n",
      "Epoch #113 Cost 397s\n",
      "Training Epoch: #114, LR: 0.0200\n",
      "[2018-06-21 09:29:50] Training Epoch [114/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.895\n",
      "[2018-06-21 09:30:17] Testing Epoch [114/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.858\n",
      "\n",
      "Epoch #114 Cost 427s\n",
      "Training Epoch: #115, LR: 0.0200\n",
      "[2018-06-21 09:37:02] Training Epoch [115/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.897\n",
      "[2018-06-21 09:37:31] Testing Epoch [115/200] Iter[124/125]\t\tLoss: 0.0041 Acc@1: 0.861\n",
      "\n",
      "Epoch #115 Cost 433s\n",
      "Training Epoch: #116, LR: 0.0200\n",
      "[2018-06-21 09:43:52] Training Epoch [116/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.898\n",
      "[2018-06-21 09:44:17] Testing Epoch [116/200] Iter[124/125]\t\tLoss: 0.0047 Acc@1: 0.844\n",
      "\n",
      "Epoch #116 Cost 406s\n",
      "Training Epoch: #117, LR: 0.0200\n",
      "[2018-06-21 09:50:53] Training Epoch [117/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.897\n",
      "[2018-06-21 09:51:21] Testing Epoch [117/200] Iter[124/125]\t\tLoss: 0.0045 Acc@1: 0.859\n",
      "\n",
      "Epoch #117 Cost 423s\n",
      "Training Epoch: #118, LR: 0.0200\n",
      "[2018-06-21 09:57:45] Training Epoch [118/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.898\n",
      "[2018-06-21 09:58:09] Testing Epoch [118/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.857\n",
      "\n",
      "Epoch #118 Cost 408s\n",
      "Training Epoch: #119, LR: 0.0200\n",
      "[2018-06-21 10:04:21] Training Epoch [119/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.896\n",
      "[2018-06-21 10:04:44] Testing Epoch [119/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.855\n",
      "\n",
      "Epoch #119 Cost 395s\n",
      "Training Epoch: #120, LR: 0.0200\n",
      "[2018-06-21 10:11:43] Training Epoch [120/200] Iter[390/390]\t\tLoss: 0.0023 Acc@1: 0.899\n",
      "[2018-06-21 10:12:09] Testing Epoch [120/200] Iter[124/125]\t\tLoss: 0.0041 Acc@1: 0.864\n",
      "\n",
      "Epoch #120 Cost 444s\n",
      "Training Epoch: #121, LR: 0.0040\n",
      "[2018-06-21 10:18:44] Training Epoch [121/200] Iter[390/390]\t\tLoss: 0.0016 Acc@1: 0.934\n",
      "[2018-06-21 10:19:09] Testing Epoch [121/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.905\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #121 Cost 420s\n",
      "Training Epoch: #122, LR: 0.0040\n",
      "[2018-06-21 10:25:25] Training Epoch [122/200] Iter[390/390]\t\tLoss: 0.0013 Acc@1: 0.944\n",
      "[2018-06-21 10:25:51] Testing Epoch [122/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.907\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #122 Cost 401s\n",
      "Training Epoch: #123, LR: 0.0040\n",
      "[2018-06-21 10:32:13] Training Epoch [123/200] Iter[390/390]\t\tLoss: 0.0012 Acc@1: 0.947\n",
      "[2018-06-21 10:32:40] Testing Epoch [123/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.909\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #123 Cost 408s\n",
      "Training Epoch: #124, LR: 0.0040\n",
      "[2018-06-21 10:38:52] Training Epoch [124/200] Iter[390/390]\t\tLoss: 0.0012 Acc@1: 0.950\n",
      "[2018-06-21 10:39:18] Testing Epoch [124/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.908\n",
      "\n",
      "Epoch #124 Cost 397s\n",
      "Training Epoch: #125, LR: 0.0040\n",
      "[2018-06-21 10:45:26] Training Epoch [125/200] Iter[390/390]\t\tLoss: 0.0011 Acc@1: 0.953\n",
      "[2018-06-21 10:45:50] Testing Epoch [125/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.907\n",
      "\n",
      "Epoch #125 Cost 392s\n",
      "Training Epoch: #126, LR: 0.0040\n",
      "[2018-06-21 10:51:59] Training Epoch [126/200] Iter[390/390]\t\tLoss: 0.0011 Acc@1: 0.956\n",
      "[2018-06-21 10:52:25] Testing Epoch [126/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.908\n",
      "\n",
      "Epoch #126 Cost 394s\n",
      "Training Epoch: #127, LR: 0.0040\n",
      "[2018-06-21 10:58:39] Training Epoch [127/200] Iter[390/390]\t\tLoss: 0.0010 Acc@1: 0.955\n",
      "[2018-06-21 10:59:05] Testing Epoch [127/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.907\n",
      "\n",
      "Epoch #127 Cost 399s\n",
      "Training Epoch: #128, LR: 0.0040\n",
      "[2018-06-21 11:05:27] Training Epoch [128/200] Iter[390/390]\t\tLoss: 0.0010 Acc@1: 0.957\n",
      "[2018-06-21 11:05:53] Testing Epoch [128/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.910\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #128 Cost 408s\n",
      "Training Epoch: #129, LR: 0.0040\n",
      "[2018-06-21 11:12:11] Training Epoch [129/200] Iter[390/390]\t\tLoss: 0.0010 Acc@1: 0.959\n",
      "[2018-06-21 11:12:36] Testing Epoch [129/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.909\n",
      "\n",
      "Epoch #129 Cost 402s\n",
      "Training Epoch: #130, LR: 0.0040\n",
      "[2018-06-21 11:19:02] Training Epoch [130/200] Iter[390/390]\t\tLoss: 0.0009 Acc@1: 0.960\n",
      "[2018-06-21 11:19:41] Testing Epoch [130/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.907\n",
      "\n",
      "Epoch #130 Cost 425s\n",
      "Training Epoch: #131, LR: 0.0040\n",
      "[2018-06-21 11:26:02] Training Epoch [131/200] Iter[390/390]\t\tLoss: 0.0009 Acc@1: 0.960\n",
      "[2018-06-21 11:26:26] Testing Epoch [131/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.910\n",
      "\n",
      "Epoch #131 Cost 405s\n",
      "Training Epoch: #132, LR: 0.0040\n",
      "[2018-06-21 11:32:36] Training Epoch [132/200] Iter[390/390]\t\tLoss: 0.0009 Acc@1: 0.961\n",
      "[2018-06-21 11:32:59] Testing Epoch [132/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.906\n",
      "\n",
      "Epoch #132 Cost 392s\n",
      "Training Epoch: #133, LR: 0.0040\n",
      "[2018-06-21 11:39:17] Training Epoch [133/200] Iter[390/390]\t\tLoss: 0.0009 Acc@1: 0.963\n",
      "[2018-06-21 11:39:41] Testing Epoch [133/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.906\n",
      "\n",
      "Epoch #133 Cost 402s\n",
      "Training Epoch: #134, LR: 0.0040\n",
      "[2018-06-21 11:45:41] Training Epoch [134/200] Iter[390/390]\t\tLoss: 0.0009 Acc@1: 0.963\n",
      "[2018-06-21 11:46:06] Testing Epoch [134/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.907\n",
      "\n",
      "Epoch #134 Cost 384s\n",
      "Training Epoch: #135, LR: 0.0040\n",
      "[2018-06-21 11:52:16] Training Epoch [135/200] Iter[390/390]\t\tLoss: 0.0009 Acc@1: 0.963\n",
      "[2018-06-21 11:52:41] Testing Epoch [135/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.908\n",
      "\n",
      "Epoch #135 Cost 395s\n",
      "Training Epoch: #136, LR: 0.0040\n",
      "[2018-06-21 11:58:53] Training Epoch [136/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.965\n",
      "[2018-06-21 11:59:18] Testing Epoch [136/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.907\n",
      "\n",
      "Epoch #136 Cost 396s\n",
      "Training Epoch: #137, LR: 0.0040\n",
      "[2018-06-21 12:05:27] Training Epoch [137/200] Iter[390/390]\t\tLoss: 0.0009 Acc@1: 0.962\n",
      "[2018-06-21 12:05:52] Testing Epoch [137/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.902\n",
      "\n",
      "Epoch #137 Cost 394s\n",
      "Training Epoch: #138, LR: 0.0040\n",
      "[2018-06-21 12:12:02] Training Epoch [138/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.964\n",
      "[2018-06-21 12:12:28] Testing Epoch [138/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.909\n",
      "\n",
      "Epoch #138 Cost 395s\n",
      "Training Epoch: #139, LR: 0.0040\n",
      "[2018-06-21 12:18:41] Training Epoch [139/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.967\n",
      "[2018-06-21 12:19:05] Testing Epoch [139/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.906\n",
      "\n",
      "Epoch #139 Cost 397s\n",
      "Training Epoch: #140, LR: 0.0040\n",
      "[2018-06-21 12:25:11] Training Epoch [140/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.965\n",
      "[2018-06-21 12:25:37] Testing Epoch [140/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.909\n",
      "\n",
      "Epoch #140 Cost 392s\n",
      "Training Epoch: #141, LR: 0.0040\n",
      "[2018-06-21 12:31:48] Training Epoch [141/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.965\n",
      "[2018-06-21 12:32:14] Testing Epoch [141/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.907\n",
      "\n",
      "Epoch #141 Cost 396s\n",
      "Training Epoch: #142, LR: 0.0040\n",
      "[2018-06-21 12:38:25] Training Epoch [142/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.966\n",
      "[2018-06-21 12:38:50] Testing Epoch [142/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.905\n",
      "\n",
      "Epoch #142 Cost 396s\n",
      "Training Epoch: #143, LR: 0.0040\n",
      "[2018-06-21 12:44:56] Training Epoch [143/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.967\n",
      "[2018-06-21 12:45:21] Testing Epoch [143/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.907\n",
      "\n",
      "Epoch #143 Cost 390s\n",
      "Training Epoch: #144, LR: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-21 12:51:33] Training Epoch [144/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.966\n",
      "[2018-06-21 12:51:59] Testing Epoch [144/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.907\n",
      "\n",
      "Epoch #144 Cost 398s\n",
      "Training Epoch: #145, LR: 0.0040\n",
      "[2018-06-21 12:58:09] Training Epoch [145/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.967\n",
      "[2018-06-21 12:58:33] Testing Epoch [145/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.906\n",
      "\n",
      "Epoch #145 Cost 394s\n",
      "Training Epoch: #146, LR: 0.0040\n",
      "[2018-06-21 13:04:42] Training Epoch [146/200] Iter[390/390]\t\tLoss: 0.0007 Acc@1: 0.968\n",
      "[2018-06-21 13:05:07] Testing Epoch [146/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.905\n",
      "\n",
      "Epoch #146 Cost 393s\n",
      "Training Epoch: #147, LR: 0.0040\n",
      "[2018-06-21 13:11:14] Training Epoch [147/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.968\n",
      "[2018-06-21 13:11:39] Testing Epoch [147/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.905\n",
      "\n",
      "Epoch #147 Cost 392s\n",
      "Training Epoch: #148, LR: 0.0040\n",
      "[2018-06-21 13:17:47] Training Epoch [148/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.967\n",
      "[2018-06-21 13:18:12] Testing Epoch [148/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.905\n",
      "\n",
      "Epoch #148 Cost 392s\n",
      "Training Epoch: #149, LR: 0.0040\n",
      "[2018-06-21 13:24:21] Training Epoch [149/200] Iter[390/390]\t\tLoss: 0.0007 Acc@1: 0.969\n",
      "[2018-06-21 13:24:46] Testing Epoch [149/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.906\n",
      "\n",
      "Epoch #149 Cost 394s\n",
      "Training Epoch: #150, LR: 0.0040\n",
      "[2018-06-21 13:30:59] Training Epoch [150/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.967\n",
      "[2018-06-21 13:31:24] Testing Epoch [150/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.902\n",
      "\n",
      "Epoch #150 Cost 398s\n",
      "Training Epoch: #151, LR: 0.0040\n",
      "[2018-06-21 13:37:38] Training Epoch [151/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.968\n",
      "[2018-06-21 13:38:02] Testing Epoch [151/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.907\n",
      "\n",
      "Epoch #151 Cost 398s\n",
      "Training Epoch: #152, LR: 0.0040\n",
      "[2018-06-21 13:44:24] Training Epoch [152/200] Iter[390/390]\t\tLoss: 0.0007 Acc@1: 0.968\n",
      "[2018-06-21 13:44:47] Testing Epoch [152/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.904\n",
      "\n",
      "Epoch #152 Cost 404s\n",
      "Training Epoch: #153, LR: 0.0040\n",
      "[2018-06-21 13:51:06] Training Epoch [153/200] Iter[390/390]\t\tLoss: 0.0007 Acc@1: 0.968\n",
      "[2018-06-21 13:51:30] Testing Epoch [153/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.907\n",
      "\n",
      "Epoch #153 Cost 403s\n",
      "Training Epoch: #154, LR: 0.0040\n",
      "[2018-06-21 13:57:55] Training Epoch [154/200] Iter[390/390]\t\tLoss: 0.0007 Acc@1: 0.969\n",
      "[2018-06-21 13:58:20] Testing Epoch [154/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.907\n",
      "\n",
      "Epoch #154 Cost 409s\n",
      "Training Epoch: #155, LR: 0.0040\n",
      "[2018-06-21 14:04:45] Training Epoch [155/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.965\n",
      "[2018-06-21 14:05:11] Testing Epoch [155/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.906\n",
      "\n",
      "Epoch #155 Cost 411s\n",
      "Training Epoch: #156, LR: 0.0040\n",
      "[2018-06-21 14:11:26] Training Epoch [156/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.967\n",
      "[2018-06-21 14:11:52] Testing Epoch [156/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.905\n",
      "\n",
      "Epoch #156 Cost 400s\n",
      "Training Epoch: #157, LR: 0.0040\n",
      "[2018-06-21 14:18:14] Training Epoch [157/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.967\n",
      "[2018-06-21 14:18:40] Testing Epoch [157/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.905\n",
      "\n",
      "Epoch #157 Cost 407s\n",
      "Training Epoch: #158, LR: 0.0040\n",
      "[2018-06-21 14:24:52] Training Epoch [158/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.968\n",
      "[2018-06-21 14:25:17] Testing Epoch [158/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.904\n",
      "\n",
      "Epoch #158 Cost 396s\n",
      "Training Epoch: #159, LR: 0.0040\n",
      "[2018-06-21 14:31:35] Training Epoch [159/200] Iter[390/390]\t\tLoss: 0.0007 Acc@1: 0.969\n",
      "[2018-06-21 14:32:01] Testing Epoch [159/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.903\n",
      "\n",
      "Epoch #159 Cost 404s\n",
      "Training Epoch: #160, LR: 0.0040\n",
      "[2018-06-21 14:38:14] Training Epoch [160/200] Iter[390/390]\t\tLoss: 0.0008 Acc@1: 0.968\n",
      "[2018-06-21 14:38:40] Testing Epoch [160/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.903\n",
      "\n",
      "Epoch #160 Cost 398s\n",
      "Training Epoch: #161, LR: 0.0008\n",
      "[2018-06-21 14:44:55] Training Epoch [161/200] Iter[390/390]\t\tLoss: 0.0005 Acc@1: 0.979\n",
      "[2018-06-21 14:45:21] Testing Epoch [161/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.913\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #161 Cost 401s\n",
      "Training Epoch: #162, LR: 0.0008\n",
      "[2018-06-21 14:51:34] Training Epoch [162/200] Iter[390/390]\t\tLoss: 0.0004 Acc@1: 0.984\n",
      "[2018-06-21 14:52:00] Testing Epoch [162/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.914\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #162 Cost 399s\n",
      "Training Epoch: #163, LR: 0.0008\n",
      "[2018-06-21 14:58:15] Training Epoch [163/200] Iter[390/390]\t\tLoss: 0.0004 Acc@1: 0.984\n",
      "[2018-06-21 14:58:41] Testing Epoch [163/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.917\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #163 Cost 400s\n",
      "Training Epoch: #164, LR: 0.0008\n",
      "[2018-06-21 15:04:55] Training Epoch [164/200] Iter[390/390]\t\tLoss: 0.0004 Acc@1: 0.986\n",
      "[2018-06-21 15:05:21] Testing Epoch [164/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.916\n",
      "\n",
      "Epoch #164 Cost 399s\n",
      "Training Epoch: #165, LR: 0.0008\n",
      "[2018-06-21 15:11:32] Training Epoch [165/200] Iter[390/390]\t\tLoss: 0.0004 Acc@1: 0.986\n",
      "[2018-06-21 15:11:55] Testing Epoch [165/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.916\n",
      "\n",
      "Epoch #165 Cost 394s\n",
      "Training Epoch: #166, LR: 0.0008\n",
      "[2018-06-21 15:18:23] Training Epoch [166/200] Iter[390/390]\t\tLoss: 0.0004 Acc@1: 0.987\n",
      "[2018-06-21 15:18:49] Testing Epoch [166/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.917\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #166 Cost 414s\n",
      "Training Epoch: #167, LR: 0.0008\n",
      "[2018-06-21 15:25:13] Training Epoch [167/200] Iter[390/390]\t\tLoss: 0.0004 Acc@1: 0.987\n",
      "[2018-06-21 15:25:39] Testing Epoch [167/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.919\n",
      "Saving Model...\n",
      "\n",
      "\n",
      "Epoch #167 Cost 409s\n",
      "Training Epoch: #168, LR: 0.0008\n",
      "[2018-06-21 15:31:58] Training Epoch [168/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.988\n",
      "[2018-06-21 15:32:24] Testing Epoch [168/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.917\n",
      "\n",
      "Epoch #168 Cost 403s\n",
      "Training Epoch: #169, LR: 0.0008\n",
      "[2018-06-21 15:38:45] Training Epoch [169/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.989\n",
      "[2018-06-21 15:39:11] Testing Epoch [169/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.915\n",
      "\n",
      "Epoch #169 Cost 407s\n",
      "Training Epoch: #170, LR: 0.0008\n",
      "[2018-06-21 15:45:38] Training Epoch [170/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.989\n",
      "[2018-06-21 15:46:04] Testing Epoch [170/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.916\n",
      "\n",
      "Epoch #170 Cost 412s\n",
      "Training Epoch: #171, LR: 0.0008\n",
      "[2018-06-21 15:52:19] Training Epoch [171/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.989\n",
      "[2018-06-21 15:52:43] Testing Epoch [171/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.917\n",
      "\n",
      "Epoch #171 Cost 399s\n",
      "Training Epoch: #172, LR: 0.0008\n",
      "[2018-06-21 15:58:58] Training Epoch [172/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.990\n",
      "[2018-06-21 15:59:23] Testing Epoch [172/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.917\n",
      "\n",
      "Epoch #172 Cost 399s\n",
      "Training Epoch: #173, LR: 0.0008\n",
      "[2018-06-21 16:05:40] Training Epoch [173/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.990\n",
      "[2018-06-21 16:06:06] Testing Epoch [173/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.917\n",
      "\n",
      "Epoch #173 Cost 402s\n",
      "Training Epoch: #174, LR: 0.0008\n",
      "[2018-06-21 16:12:20] Training Epoch [174/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.990\n",
      "[2018-06-21 16:12:45] Testing Epoch [174/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.916\n",
      "\n",
      "Epoch #174 Cost 399s\n",
      "Training Epoch: #175, LR: 0.0008\n",
      "[2018-06-21 16:19:01] Training Epoch [175/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.990\n",
      "[2018-06-21 16:19:26] Testing Epoch [175/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.915\n",
      "\n",
      "Epoch #175 Cost 401s\n",
      "Training Epoch: #176, LR: 0.0008\n",
      "[2018-06-21 16:25:47] Training Epoch [176/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.990\n",
      "[2018-06-21 16:26:13] Testing Epoch [176/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.916\n",
      "\n",
      "Epoch #176 Cost 406s\n",
      "Training Epoch: #177, LR: 0.0008\n",
      "[2018-06-21 16:32:35] Training Epoch [177/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.989\n",
      "[2018-06-21 16:32:59] Testing Epoch [177/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.918\n",
      "\n",
      "Epoch #177 Cost 406s\n",
      "Training Epoch: #178, LR: 0.0008\n",
      "[2018-06-21 16:39:23] Training Epoch [178/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.990\n",
      "[2018-06-21 16:39:49] Testing Epoch [178/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.914\n",
      "\n",
      "Epoch #178 Cost 410s\n",
      "Training Epoch: #179, LR: 0.0008\n",
      "[2018-06-21 16:46:08] Training Epoch [179/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.991\n",
      "[2018-06-21 16:46:34] Testing Epoch [179/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.916\n",
      "\n",
      "Epoch #179 Cost 404s\n",
      "Training Epoch: #180, LR: 0.0008\n",
      "[2018-06-21 16:52:51] Training Epoch [180/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.991\n",
      "[2018-06-21 16:53:17] Testing Epoch [180/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.917\n",
      "\n",
      "Epoch #180 Cost 402s\n",
      "Training Epoch: #181, LR: 0.0008\n",
      "[2018-06-21 16:58:53] Training Epoch [181/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.991\n",
      "[2018-06-21 16:59:11] Testing Epoch [181/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.917\n",
      "\n",
      "Epoch #181 Cost 354s\n",
      "Training Epoch: #182, LR: 0.0008\n",
      "[2018-06-21 17:03:59] Training Epoch [182/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.991\n",
      "[2018-06-21 17:04:17] Testing Epoch [182/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.917\n",
      "\n",
      "Epoch #182 Cost 306s\n",
      "Training Epoch: #183, LR: 0.0008\n",
      "[2018-06-21 17:09:17] Training Epoch [183/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.991\n",
      "[2018-06-21 17:09:41] Testing Epoch [183/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.917\n",
      "\n",
      "Epoch #183 Cost 323s\n",
      "Training Epoch: #184, LR: 0.0008\n",
      "[2018-06-21 17:15:52] Training Epoch [184/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.992\n",
      "[2018-06-21 17:16:19] Testing Epoch [184/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.917\n",
      "\n",
      "Epoch #184 Cost 398s\n",
      "Training Epoch: #185, LR: 0.0008\n",
      "[2018-06-21 17:22:48] Training Epoch [185/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.991\n",
      "[2018-06-21 17:23:16] Testing Epoch [185/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.915\n",
      "\n",
      "Epoch #185 Cost 416s\n",
      "Training Epoch: #186, LR: 0.0008\n",
      "[2018-06-21 17:29:40] Training Epoch [186/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.991\n",
      "[2018-06-21 17:30:06] Testing Epoch [186/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.916\n",
      "\n",
      "Epoch #186 Cost 410s\n",
      "Training Epoch: #187, LR: 0.0008\n",
      "[2018-06-21 17:36:37] Training Epoch [187/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.992\n",
      "[2018-06-21 17:37:03] Testing Epoch [187/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.915\n",
      "\n",
      "Epoch #187 Cost 416s\n",
      "Training Epoch: #188, LR: 0.0008\n",
      "[2018-06-21 17:43:53] Training Epoch [188/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.993\n",
      "[2018-06-21 17:44:19] Testing Epoch [188/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.916\n",
      "\n",
      "Epoch #188 Cost 436s\n",
      "Training Epoch: #189, LR: 0.0008\n",
      "[2018-06-21 17:50:42] Training Epoch [189/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 17:51:08] Testing Epoch [189/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.917\n",
      "\n",
      "Epoch #189 Cost 408s\n",
      "Training Epoch: #190, LR: 0.0008\n",
      "[2018-06-21 17:57:27] Training Epoch [190/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 17:57:53] Testing Epoch [190/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.916\n",
      "\n",
      "Epoch #190 Cost 405s\n",
      "Training Epoch: #191, LR: 0.0008\n",
      "[2018-06-21 18:04:02] Training Epoch [191/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 18:04:28] Testing Epoch [191/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.916\n",
      "\n",
      "Epoch #191 Cost 394s\n",
      "Training Epoch: #192, LR: 0.0008\n",
      "[2018-06-21 18:10:39] Training Epoch [192/200] Iter[390/390]\t\tLoss: 0.0003 Acc@1: 0.992\n",
      "[2018-06-21 18:11:05] Testing Epoch [192/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.916\n",
      "\n",
      "Epoch #192 Cost 397s\n",
      "Training Epoch: #193, LR: 0.0008\n",
      "[2018-06-21 18:17:16] Training Epoch [193/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 18:17:41] Testing Epoch [193/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.917\n",
      "\n",
      "Epoch #193 Cost 396s\n",
      "Training Epoch: #194, LR: 0.0008\n",
      "[2018-06-21 18:24:22] Training Epoch [194/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 18:24:47] Testing Epoch [194/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.918\n",
      "\n",
      "Epoch #194 Cost 426s\n",
      "Training Epoch: #195, LR: 0.0008\n",
      "[2018-06-21 18:30:59] Training Epoch [195/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 18:31:24] Testing Epoch [195/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.917\n",
      "\n",
      "Epoch #195 Cost 396s\n",
      "Training Epoch: #196, LR: 0.0008\n",
      "[2018-06-21 18:37:33] Training Epoch [196/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 18:37:56] Testing Epoch [196/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.917\n",
      "\n",
      "Epoch #196 Cost 392s\n",
      "Training Epoch: #197, LR: 0.0008\n",
      "[2018-06-21 18:44:04] Training Epoch [197/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 18:44:30] Testing Epoch [197/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.915\n",
      "\n",
      "Epoch #197 Cost 394s\n",
      "Training Epoch: #198, LR: 0.0008\n",
      "[2018-06-21 18:50:38] Training Epoch [198/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 18:51:03] Testing Epoch [198/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.916\n",
      "\n",
      "Epoch #198 Cost 393s\n",
      "Training Epoch: #199, LR: 0.0008\n",
      "[2018-06-21 18:57:12] Training Epoch [199/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.993\n",
      "[2018-06-21 18:57:38] Testing Epoch [199/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.916\n",
      "\n",
      "Epoch #199 Cost 394s\n",
      "Training Epoch: #200, LR: 0.0008\n",
      "[2018-06-21 19:03:47] Training Epoch [200/200] Iter[390/390]\t\tLoss: 0.0002 Acc@1: 0.992\n",
      "[2018-06-21 19:04:12] Testing Epoch [200/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.917\n",
      "\n",
      "Epoch #200 Cost 394s\n",
      "Best Acc@1: 91.8700\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for _epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    start_time = time.time()\n",
    "    train(_epoch)\n",
    "    print()\n",
    "    test(_epoch)\n",
    "    print()\n",
    "    print()\n",
    "    end_time   = time.time()\n",
    "    print('Epoch #%d Cost %ds' % (_epoch, end_time - start_time))\n",
    "    \n",
    "print('Best Acc@1: %.4f' % (best_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
